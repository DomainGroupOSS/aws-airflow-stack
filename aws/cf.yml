AWSTemplateFormatVersion: 2010-09-09
Description: The Airflow cluster stack
Parameters:
  Application:
    Type: String
    Description: The application variable
    Default: Airflow
  Team:
    Type: String
    Description: The team
    Default: Data Team
  Environment:
    Type: String
    Description: The environment variable
    Default: Staging
  Domain:
    Type: String
    Description: The domain suffix
    Default: insights.domain.com.au
  VPC:
    Description: The vpc it is in
    Type: String
    Default: vpc-0900dec7801613f72
  VpcCidrBlock:
    Description: The IPv4 CIDR block to be used in the VPC.
    Type: String
    AllowedPattern: >-
      ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/([0-9]|[1-2][0-9]|3[0-2]))$
    Default: 10.138.32.0/20
  DummySubnetBlock:
    Description: The IPv4 CIDR block to be used in the dummy subnet.
    Type: String
    AllowedPattern: >-
      ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/([0-9]|[1-2][0-9]|3[0-2]))$
    Default: 10.0.99.0/24
  StackSubnet:
    Description: Stack subnet
    Type: String
    Default: subnet-0281813787e9e6370
  DBSubnets:
    Description: Stack subnet
    Type: CommaDelimitedList
    Default: subnet-0281813787e9e6370,subnet-02b6717a06309435c
  StackSubnetBlock:
    Description: The IPv4 CIDR block to be used in the stack subnet.
    Type: String
    AllowedPattern: >-
      ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/([0-9]|[1-2][0-9]|3[0-2]))$
    Default: 10.0.0.0/24
  AllowedSshBlock:
    Description: >-
      The IPv4 CIDR block to allow SSH access in all machines. The default of
      0.0.0.0/0 allows SSH from everywhere, which is convenient but less secure.
    Type: String
    AllowedPattern: >-
      ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/([0-9]|[1-2][0-9]|3[0-2]))$
    Default: 10.0.0.0/8
  AllowedWebBlock:
    Description: >-
      The IPv4 CIDR block to allow HTTP access in the webserver. The default of
      0.0.0.0/0 allows HTTP from everywhere, which is convenient but less
      secure.
    Type: String
    AllowedPattern: >-
      ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/([0-9]|[1-2][0-9]|3[0-2]))$
    Default:  10.0.0.0/8
  WebserverPort:
    Description: >-
      The port Airflow webserver will be listening. Ports below 1024 can be
      opened only with root privileges and the airflow process does not run as
      such.
    Type: String
    Default: 8080
  KeyPair:
    Description: Amazon EC2 Key Pair to be used paired with the EC2 instances.
    Type: 'AWS::EC2::KeyPair::KeyName'
  SchedulerInstanceType:
    Description: EC2 instance type to use for the scheduler.
    Type: String
    Default: t3.medium
  WebserverInstanceType:
    Description: EC2 instance type to use for the webserver.
    Type: String
    Default: t3.small
  WorkerInstanceType:
    Description: EC2 instance type to use for the workers.
    Type: String
    Default: t3.micro
  WorkerConcurrency:
    Description: Number of concurrent units per machine.
    Type: Number
    Default: 3
  MinGroupSize:
    Description: The minimum number of active worker instances.
    Type: Number
    Default: 0
  MaxGroupSize:
    Description: The maximum number of active worker instances.
    Type: Number
    Default: 40
  ShrinkThreshold:
    Description: >-
      The timeout (in seconds, multiple of 60) after which the queue staying
      empty will trigger the AutoScaling group to Scale In, deallocating one
      worker instance.
    Type: Number
    Default: 0.5
  GrowthThreshold:
    Description: >-
      The threshold for the average queue size from which going equal or above
      will trigger the AutoScaling group to Scale Out, allocating one worker
      instance.
    Type: Number
    Default: 0.9
  DbMasterUsername:
    Description: The username to be used in the airflow database.
    Type: String
    Default: airflow
  DbMasterPassword:
    Description: The password to be used in the airflow database.
    Type: String
    NoEcho: true
  LoadExampleDags:
    Description: >-
      Load the example DAGs distributed with Airflow. Useful if deploying a
      stack for demonstrating a few topologies, operators and scheduling
      strategies.
    Type: String
    AllowedValues:
      - 'False'
      - 'True'
    Default: 'False'
  LoadDefaultConns:
    Description: >-
      Load the default connections initialized by Airflow. Most consider these
      unnecessary, which is why the default is to not load them.
    Type: String
    AllowedValues:
      - 'False'
      - 'True'
    Default: 'False'
Resources:
  AirflowScheduler:
    Type: 'AWS::EC2::Instance'
    Properties:
      InstanceType: !Ref SchedulerInstanceType
      ImageId: ami-0b8dea0e70b969adc # HVMGP2
      SubnetId: !Ref StackSubnet
      KeyName: !Ref KeyPair
      SecurityGroupIds:
        - !Ref Control
      IamInstanceProfile: !Ref AirflowProfile
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-scheduler'
      UserData: !Base64
        'Fn::Sub': |
          #!/bin/bash -xe
          echo $'TURBINE_MACHINE=SCHEDULER' > /etc/environment
          /opt/aws/bin/cfn-init -v \
            --region ${AWS::Region} \
            --stack ${AWS::StackName} \
            --resource Meta
          /opt/aws/bin/cfn-signal -e $?
            --region ${AWS::Region} \
            --stack ${AWS::StackName} \
            --resource AirflowScheduler
    DependsOn:
      - EfsMountTarget
      - Meta
  AirflowWebserverTemp:
    Type: 'AWS::EC2::Instance'
    Properties:
      InstanceType: !Ref WebserverInstanceType
      ImageId: ami-0b8dea0e70b969adc # HVMGP2
      SubnetId: !Ref StackSubnet
      KeyName: !Ref KeyPair
      SecurityGroupIds:
        - !Ref Web
      IamInstanceProfile: !Ref AirflowProfile
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-webserver'
      UserData: !Base64
        'Fn::Sub': |
          #!/bin/bash -xe
          echo $'TURBINE_MACHINE=WEBSERVER' > /etc/environment
          /opt/aws/bin/cfn-init -v \
            --region ${AWS::Region} \
            --stack ${AWS::StackName} \
            --resource Meta
          /opt/aws/bin/cfn-signal -e $?
            --region ${AWS::Region} \
            --stack ${AWS::StackName} \
            --resource AirflowWebserver
          echo "Update R53 entry"
          HOSTED_ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${Domain}. | jq -r '.HostedZones[].Id' | awk -F'/' '{print $3}')
          echo '{
            "Comment": "Update new IP",
            "Changes": [
              {
                "Action": "UPSERT",
                "ResourceRecordSet": {
                  "Name": "${AWS::StackName}-webserver.${Domain}",
                  "Type": "A",
                  "TTL": 300,
                  "ResourceRecords": [
                    {
                      "Value": "'$HOST'"
                    }
                  ]
                }
              }
            ]
          }' > /tmp/record.json
          aws route53 change-resource-record-sets --hosted-zone-id "$HOSTED_ZONE_ID" --change-batch file:///tmp//tmp/record.json
    DependsOn:
      - EfsMountTarget
      - Meta
  AirflowWebServerLaunchConfig:
    Type: 'AWS::AutoScaling::LaunchConfiguration'
    Properties:
      ImageId: ami-0b8dea0e70b969adc #hvmgp2
      InstanceType: !Ref WebserverInstanceType
      KeyName: !Ref KeyPair
      SecurityGroups:
        - !Ref Web
      IamInstanceProfile: !Ref AirflowProfile
      UserData: !Base64
        'Fn::Sub': |
          #!/bin/bash -xe
          echo $'TURBINE_MACHINE=WEBSERVER' > /etc/environment
          /opt/aws/bin/cfn-init -v \
            --region ${AWS::Region} \
            --stack ${AWS::StackName} \
            --resource Meta
          /opt/aws/bin/cfn-signal -e $?
            --region ${AWS::Region} \
            --stack ${AWS::StackName} \
            --resource AirflowWebserver
          echo "Update R53 entry"
          HOSTED_ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${Domain}. | jq -r '.HostedZones[].Id' | awk -F'/' '{print $3}')
          echo '{
            "Comment": "Update new IP",
            "Changes": [
              {
                "Action": "UPSERT",
                "ResourceRecordSet": {
                  "Name": "${AWS::StackName}-webserver.${Domain}",
                  "Type": "A",
                  "TTL": 300,
                  "ResourceRecords": [
                    {
                      "Value": "'$HOST'"
                    }
                  ]
                }
              }
            ]
          }' > /tmp/record.json
          aws route53 change-resource-record-sets --hosted-zone-id "$HOSTED_ZONE_ID" --change-batch file:///tmp//tmp/record.json
    DependsOn:
      - Meta
      - EfsMountTarget
  AirflowWebServerAutoScalingGroup:
    Type: 'AWS::AutoScaling::AutoScalingGroup'
    Properties:
      AutoScalingGroupName: !Sub '${AWS::StackName}-webserver-scaling-group'
      LaunchConfigurationName: !Ref AirflowWebServerLaunchConfig
      MinSize: 0
      MaxSize: 0
      MetricsCollection:
        - Granularity: 1Minute
      VPCZoneIdentifier:
        - !Ref StackSubnet
      Tags:
        - Key: Environment
          Value: !Ref Environment
          PropagateAtLaunch: true
        - Key: Application
          Value: !Ref Application
          PropagateAtLaunch: true
        - Key: Team
          Value: !Ref Team
          PropagateAtLaunch: true
        - Key: Name
          Value: !Sub '${AWS::StackName}-webserver'
          PropagateAtLaunch: true
  AirflowWorkerConfig:
    Type: 'AWS::AutoScaling::LaunchConfiguration'
    Properties:
      ImageId: ami-0b8dea0e70b969adc #hvmgp2
      InstanceType: !Ref WorkerInstanceType
      KeyName: !Ref KeyPair
      SecurityGroups:
        - !Ref Comms
      IamInstanceProfile: !Ref AirflowProfile
      UserData: !Base64
        'Fn::Sub': |
          #!/bin/bash -xe
          echo $'TURBINE_MACHINE=WORKER' > /etc/environment
          /opt/aws/bin/cfn-init -v \
            --region ${AWS::Region} \
            --stack ${AWS::StackName} \
            --resource Meta
    
    DependsOn:
      - Meta
  AutoScalingGroup:
    Type: 'AWS::AutoScaling::AutoScalingGroup'
    Properties:
      AutoScalingGroupName: !Sub '${AWS::StackName}-worker-scaling-group'
      LaunchConfigurationName: !Ref AirflowWorkerConfig
      MinSize: !Ref MinGroupSize
      MaxSize: !Ref MaxGroupSize
      MetricsCollection:
        - Granularity: 1Minute
      VPCZoneIdentifier:
        - !Ref StackSubnet
      TerminationPolicies:
        - OldestLaunchConfiguration
        - OldestLaunchTemplate
        - ClosestToNextInstanceHour
        - AllocationStrategy
      Tags:
        - Key: Environment
          Value: !Ref Environment
          PropagateAtLaunch: true
        - Key: Application
          Value: !Ref Application
          PropagateAtLaunch: true
        - Key: Team
          Value: !Ref Team
          PropagateAtLaunch: true
        - Key: Name
          Value: !Sub '${AWS::StackName}-worker'
          PropagateAtLaunch: true
  AutoScalingHook:
    Type: 'AWS::AutoScaling::LifecycleHook'
    Properties:
      AutoScalingGroupName: !Ref AutoScalingGroup
      DefaultResult: CONTINUE
      HeartbeatTimeout: 300
      LifecycleHookName: !Sub '${AWS::StackName}-scaling-lfhook'
      LifecycleTransition: 'autoscaling:EC2_INSTANCE_TERMINATING'
  LogsBucket:
    Type: 'AWS::S3::Bucket'
    Properties:
      BucketName: !Sub '${AWS::StackName}-logs.${Domain}'
  DeploymentsBucket:
    Type: 'AWS::S3::Bucket'
    Properties:
      BucketName: !Sub '${AWS::StackName}-deployments.${Domain}'
  CodeDeployApplication:
    Type: 'AWS::CodeDeploy::Application'
    Properties:
      ApplicationName: !Sub '${AWS::StackName}-deployment-application'
      ComputePlatform: Server
  CodeDeployDeploymentGroup:
    Type: 'AWS::CodeDeploy::DeploymentGroup'
    Properties:
      ApplicationName: !Ref CodeDeployApplication
      DeploymentGroupName: !Sub '${AWS::StackName}-deployment-group'
      AutoScalingGroups:
        - !Ref AutoScalingGroup
      Ec2TagSet:
        Ec2TagSetList:
          - Ec2TagGroup:
              - Type: KEY_AND_VALUE
                Key: Name
                Value: !Sub '${AWS::StackName}-scheduler'
              - Type: KEY_AND_VALUE
                Key: Name
                Value: !Sub '${AWS::StackName}-webserver'
      ServiceRoleArn: !GetAtt
        - CodeDeployServiceRole
        - Arn
  CodeDeployServiceRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - codedeploy.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSCodeDeployRole'
  ScalerRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns: ["arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"]
      Policies:
        - PolicyName: !Sub '${AWS::StackName}-scaler-function-policy'
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Resource: '*'
                Action:
                  - 'autoscaling:*'
                  - 'sqs:*'
  LambdaFunctionLogGroup:
    Type: "AWS::Logs::LogGroup"
    DependsOn: "ScaleFunction"
    Properties: 
      RetentionInDays: 14
      LogGroupName: !Join ["", ["/aws/lambda/", !Ref ScaleFunction]]
  ScaleFunction:
    Type: AWS::Lambda::Function
    Properties:
      Code:
        ZipFile: |
            import boto3
            def lambda_handler(event, context):
              client = boto3.client('sqs')
              queue_url = client.get_queue_url(
                  QueueName='airflow-stage-Tasks-1SRC2B5YQV7KS',
              )['QueueUrl']
              # print(queue_url)
              size = client.get_queue_attributes(
                AttributeNames=['ApproximateNumberOfMessages'],
                QueueUrl=queue_url
              )['Attributes']['ApproximateNumberOfMessages']
              # print(size)
              best_size = int(size)//3
              client = boto3.client('autoscaling')
              response = client.describe_auto_scaling_groups(
                  AutoScalingGroupNames=[
                      'airflow-stage-worker-scaling-group',
                  ],
              )['AutoScalingGroups']
              asg = response[0]
              if best_size > int(asg['MaxSize']):
                best_size = int(asg['MaxSize'])
              current_size = int(asg['DesiredCapacity'])
              if best_size == current_size:
                print("Current size already are desired: ", best_size, ", nothing to do.")
                return {}
              if best_size != 0 and best_size < current_size: # need to scale in
                if current_size - best_size < 2: # only scale in more than 1 instance. This is to make the  scale down more stable
                  print("Won't scale in simply for one instances. Current:", asg['DesiredCapacity'], "Desired:", size)
                  return {}
              print("Adjusting desired ASG size to: ", best_size, ", existing size: ", current_size)
              client.set_desired_capacity(
                  AutoScalingGroupName='airflow-stage-worker-scaling-group',
                  DesiredCapacity=best_size,
                  HonorCooldown=False
              )
              return {}

      Handler: index.lambda_handler
      Runtime: python3.7
      Role: !GetAtt ScalerRole.Arn
      Timeout: "30"
  WebDNSRecord: # place holder for easy management
    Type: AWS::Route53::RecordSet
    Properties:
      HostedZoneName: !Sub '${Domain}.'
      Comment: DNS name for my web server
      Name: !Sub '${AWS::StackName}-webserver.${Domain}'
      Type: A
      TTL: '900'
      ResourceRecords:
      - 0.0.0.0 # initially set as 0
  Timer:
    Type: 'AWS::Events::Rule'
    Properties:
      ScheduleExpression: rate(1 minute)
      State: ENABLED
      Targets:
        - Arn: !GetAtt
            - ScaleFunction
            - Arn
          Id: TargetFunction
  Invoke:
    Type: 'AWS::Lambda::Permission'
    Properties:
      FunctionName: !Ref ScaleFunction
      Action: 'lambda:InvokeFunction'
      Principal: events.amazonaws.com
      SourceArn: !GetAtt
        - Timer
        - Arn
  Shrink:
    Type: 'AWS::CloudWatch::Alarm'
    Properties:
      AlarmActions:
        - !Ref ScaleIn
      Namespace: Turbine
      MetricName: WorkerLoad
      Dimensions:
        - Name: StackName
          Value: !Ref 'AWS::StackName'
      Statistic: Average
      Period: 60
      EvaluationPeriods: 1
      Threshold: !Ref ShrinkThreshold
      ComparisonOperator: LessThanOrEqualToThreshold
  Growth:
    Type: 'AWS::CloudWatch::Alarm'
    Properties:
      AlarmActions:
        - !Ref ScaleOut
      Namespace: Turbine
      MetricName: WorkerLoad
      Dimensions:
        - Name: StackName
          Value: !Ref 'AWS::StackName'
      Statistic: Average
      Period: 60
      EvaluationPeriods: 1
      Threshold: !Ref GrowthThreshold
      ComparisonOperator: GreaterThanOrEqualToThreshold
  ScaleIn:
    Type: 'AWS::AutoScaling::ScalingPolicy'
    Properties:
      AdjustmentType: ChangeInCapacity
      PolicyType: SimpleScaling
      ScalingAdjustment: -1
      Cooldown: '600'
      AutoScalingGroupName: !Ref AutoScalingGroup
  ScaleOut:
    Type: 'AWS::AutoScaling::ScalingPolicy'
    Properties:
      AdjustmentType: ChangeInCapacity
      PolicyType: SimpleScaling
      ScalingAdjustment: 1
      Cooldown: '90'
      AutoScalingGroupName: !Ref AutoScalingGroup
  EfsFileSystem:
    Type: 'AWS::EFS::FileSystem'
    Properties:
      FileSystemTags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-filesystem'
  EfsMountTarget:
    Type: 'AWS::EFS::MountTarget'
    Properties:
      FileSystemId: !Ref EfsFileSystem
      SubnetId: !Ref StackSubnet
      SecurityGroups:
        - !Ref Access
  DBSubnetGroup:
    Type: 'AWS::RDS::DBSubnetGroup'
    Properties:
      DBSubnetGroupDescription: Associates the Database Instances with the selected VPC Subnets.
      SubnetIds: !Ref DBSubnets
  Database:
    Type: 'AWS::RDS::DBInstance'
    DeletionPolicy: Retain
    UpdateReplacePolicy: Retain
    Properties:
      DBInstanceIdentifier: !Sub '${AWS::StackName}-db'
      # EngineMode: serverless # user this when really needs more
      AllocatedStorage: '60'
      DBInstanceClass: db.t2.small
      DBName: airflow
      Engine: postgres
      MasterUsername: !Ref DbMasterUsername
      MasterUserPassword: !Ref DbMasterPassword
      BackupRetentionPeriod: 10
      DeletionProtection: true
      DeleteAutomatedBackups: true
      PreferredBackupWindow: 17:00-18:00
      PreferredMaintenanceWindow: Sun:18:00-Sun:18:30
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-database'
      DBSubnetGroupName: !Ref DBSubnetGroup
      VPCSecurityGroups:
        - !Ref Connection
  Tasks:
    Type: 'AWS::SQS::Queue'
    Properties: {}
  Access:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: >-
        Security Rules with permissions for the shared filesystem across Airflow
        instances.
      SecurityGroupIngress:
        - CidrIp: !Ref VpcCidrBlock
          IpProtocol: TCP
          FromPort: 2049
          ToPort: 2049
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-access'
  Control:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: >-
        Security Rules with permissions for node intercommunication between
        Airflow instances and remote access.
      SecurityGroupIngress:
        - CidrIp: !Ref VpcCidrBlock
          IpProtocol: TCP
          FromPort: 8793
          ToPort: 8793
        - CidrIp: !Ref AllowedSshBlock
          IpProtocol: TCP
          FromPort: 22
          ToPort: 22
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-control'
  Comms:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: >-
        Security Rules with permissions for node itercommunication between
        Airflow worker instances.
      SecurityGroupIngress:
        - CidrIp: !Ref VpcCidrBlock
          IpProtocol: TCP
          FromPort: 8793
          ToPort: 8793
        - CidrIp: !Ref AllowedSshBlock
          IpProtocol: TCP
          FromPort: 22
          ToPort: 22
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-comms'
  Web:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: Security Rules with permissions for the web UI exposed by Airflow.
      SecurityGroupIngress:
        - CidrIp: !Ref AllowedWebBlock
          IpProtocol: TCP
          FromPort: !Ref WebserverPort
          ToPort: !Ref WebserverPort
        - CidrIp: !Ref AllowedSshBlock
          IpProtocol: TCP
          FromPort: 22
          ToPort: 22
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-web'
  Connection:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: Security Rules with permissions for database connections for Airflow.
      SecurityGroupIngress:
        - CidrIp: !Ref VpcCidrBlock
          IpProtocol: TCP
          FromPort: 5432
          ToPort: 5432
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-connection'
  AirflowRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - ec2.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      Policies:
        - PolicyName: !Sub '${AWS::StackName}-queue-rw-policy'
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - 'sqs:ListQueues'
                Resource:
                  - !Sub 'arn:aws:sqs:*:${AWS::AccountId}:*'
              - Effect: Allow
                Action:
                  - 'sqs:ChangeMessageVisibility'
                  - 'sqs:DeleteMessage'
                  - 'sqs:GetQueueAttributes'
                  - 'sqs:GetQueueUrl'
                  - 'sqs:ReceiveMessage'
                  - 'sqs:SendMessage'
                Resource: !Sub
                  - 'arn:aws:sqs:*:${AWS::AccountId}:${queue}'
                  - queue: !GetAtt
                      - Tasks
                      - QueueName
        - PolicyName: !Sub '${AWS::StackName}-deployments-r-policy'
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - 's3:Get*'
                  - 's3:List*'
                Resource: !Sub 'arn:aws:s3:::${DeploymentsBucket}/*'
              - Effect: Allow
                Action:
                  - 'codedeploy:List*'
                Resource: !Sub 'arn:aws:codedeploy:*:${AWS::AccountId}:deploymentgroup:*'
        - PolicyName: !Sub '${AWS::StackName}-logs-rw-policy'
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - 's3:Get*'
                  - 's3:Put*'
                Resource: !Sub 'arn:aws:s3:::${LogsBucket}/*'
        - PolicyName: !Sub '${AWS::StackName}-lifecycle-heartbeat'
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - 'autoscaling:RecordLifecycleActionHeartbeat'
                  - 'autoscaling:CompleteLifecycleAction'
                Resource: !Sub 'arn:aws:autoscaling:*:${AWS::AccountId}:autoScalingGroup:*:*'
              - Effect: Allow
                Action:
                  - 'autoscaling:DescribeScalingActivities'
                Resource: '*'
  AirflowProfile:
    Type: 'AWS::IAM::InstanceProfile'
    Properties:
      Roles:
        - !Ref AirflowRole
  Meta:
    Type: 'AWS::CloudFormation::WaitConditionHandle'
    Properties: {}
    Metadata:
      'AWS::CloudFormation::Init':
        configSets:
          default:
            - filesys
            - runtime
            - service
            - metahup
            - cdagent
        filesys:
          commands:
            mkdir:
              test: test ! -d /airflow
              command: |
                mkdir /airflow
                chown -R ec2-user /airflow
            mount:
              test: test ! "$(mount | grep /mnt/efs)"
              command: !Sub |
                mkdir -p /mnt/efs
                fspec="${EfsFileSystem}.efs.${AWS::Region}.amazonaws.com:/"
                param="nfsvers=4.1,rsize=1048576,wsize=1048576"
                param="$param,hard,timeo=600,retrans=2,noresvport"
                echo "$fspec /mnt/efs nfs $param,_netdev 0 0" > /etc/fstab
                mount /mnt/efs
                chown -R ec2-user /mnt/efs
        runtime:
          packages:
            yum:
              git: []
              gcc: []
              gcc-c++: []
              jq: []
              lapack-devel: []
              libcurl-devel: []
              libxml2-devel: []
              libxslt-devel: []
              openssl-devel: []
              postgresql-devel: []
              python3: []
              python3-devel: []
              python3-pip: []
              python3-wheel: []
          commands:
            install:
              command: |
                PYCURL_SSL_LIBRARY=openssl pip3 install \
                  --no-cache-dir --compile --ignore-installed \
                  pycurl
                SLUGIFY_USES_TEXT_UNIDECODE=yes pip3 install \
                  apache-airflow[celery,postgres,s3]==1.10.2 \
                  celery[sqs] \
                  billiard==3.5.0.4 \
                  tenacity==4.12.0
        service:
          files:
            /usr/bin/turbine:
              mode: 755
              content: |
                #!/bin/sh
                if [ "$TURBINE_MACHINE" == "SCHEDULER" ]
                then exec airflow scheduler
                elif [ "$TURBINE_MACHINE" == "WEBSERVER" ]
                then
                  exec airflow webserver
                elif [ "$TURBINE_MACHINE" == "WORKER" ]
                then exec airflow worker
                else echo "TURBINE_MACHINE value unknown" && exit 1
                fi
            /etc/sysconfig/airflow:
              content: !Sub
                - |
                  AWS_DEFAULT_REGION=${AWS::Region}
                  AIRFLOW_HOME=/airflow
                  AIRFLOW__CORE__EXECUTOR=CeleryExecutor
                  AIRFLOW__CORE__LOAD_EXAMPLES=${LoadExampleDags}
                  TURBINE__CORE__LOAD_DEFAULTS=${LoadDefaultConns}
                  AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql://${DbMasterUsername}:${DbMasterPassword}@${rds}/airflow
                  AIRFLOW__CORE__REMOTE_BASE_LOG_FOLDER=s3://${LogsBucket}
                  AIRFLOW__CORE__REMOTE_LOGGING=True
                  AIRFLOW__WEBSERVER__BASE_URL=http://INJECTHOST:${WebserverPort}
                  AIRFLOW__WEBSERVER__WEB_SERVER_PORT=${WebserverPort}
                  AIRFLOW__CELERY__BROKER_URL=sqs://
                  AIRFLOW__CELERY__DEFAULT_QUEUE=${queue}
                  AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${DbMasterUsername}:${DbMasterPassword}@${rds}/airflow
                  AIRFLOW__CELERY__WORKER_CONCURRENCY=${WorkerConcurrency}
                  AIRFLOW__CELERY_BROKER_TRANSPORT_OPTIONS__REGION=${AWS::Region}
                - queue: !GetAtt
                    - Tasks
                    - QueueName
                  rds: !GetAtt
                    - Database
                    - Endpoint.Address
            /usr/lib/tmpfiles.d/airflow.conf:
              content: |
                D /run/airflow 0755 ec2-user ec2-user
            /usr/lib/systemd/system/airflow.service:
              content: |
                [Service]
                EnvironmentFile=/etc/sysconfig/airflow
                User=ec2-user
                Group=ec2-user
                ExecStart=/usr/bin/turbine
                Restart=always
                RestartSec=5s
                KillMode=mixed
                TimeoutStopSec=24h
                [Install]
                WantedBy=multi-user.target
            /usr/lib/systemd/system/watcher.path:
              content: |
                [Unit]
                After=airflow.service
                PartOf=airflow.service
                [Path]
                PathModified=/etc/sysconfig/airflow
                [Install]
                WantedBy=airflow.service
            /usr/lib/systemd/system/watcher.service:
              content: |
                [Service]
                Type=oneshot
                ExecStartPre=/usr/bin/systemctl daemon-reload
                ExecStart=/usr/bin/systemctl restart airflow
            /usr/bin/lchkill:
              mode: 755
              content: !Sub |
                #!/bin/sh
                INSTANCE_ID=$(ec2-metadata -i | awk '{print $2}')
                TERMINATE_MESSAGE="Terminating EC2 instance: $INSTANCE_ID"
                TERMINATING=$(aws autoscaling describe-scaling-activities \
                  --auto-scaling-group-name '${AWS::StackName}-scaling-group' \
                  --max-items 100 \
                  --region '${AWS::Region}' | \
                  jq --arg TERMINATE_MESSAGE "$TERMINATE_MESSAGE" \
                  '.Activities[]
                  | select(.Description
                  | test($TERMINATE_MESSAGE)) != []')

                if [ "$TERMINATING" = "true" ]
                then
                  systemctl stop airflow
                fi
            /usr/lib/systemd/system/lchkill.timer:
              content: |
                [Timer]
                OnCalendar=*:0/1
                [Install]
                WantedBy=airflow.service
            /usr/lib/systemd/system/lchkill.service:
              content: |
                [Service]
                Type=oneshot
                ExecStart=/usr/bin/lchkill
            /usr/bin/lchbeat:
              mode: 755
              content: !Sub |
                #!/bin/sh
                SERVICE_STATUS=$(systemctl is-active airflow)

                if [ "$SERVICE_STATUS" = "deactivating" ]
                then
                  aws autoscaling record-lifecycle-action-heartbeat \
                    --instance-id $(ec2-metadata -i | awk '{print $2}') \
                    --lifecycle-hook-name '${AWS::StackName}-scaling-lfhook' \
                    --auto-scaling-group-name '${AWS::StackName}-scaling-group' \
                    --region '${AWS::Region}'
                fi
            /usr/lib/systemd/system/lchbeat.timer:
              content: |
                [Timer]
                OnCalendar=*:0/1
                [Install]
                WantedBy=airflow.service
            /usr/lib/systemd/system/lchbeat.service:
              content: |
                [Service]
                Type=oneshot
                ExecStart=/usr/bin/lchbeat
          commands:
            setup:
              command: !Sub |
                cat /etc/environment >> /etc/sysconfig/airflow

                PUBLIC=$(curl -s -o /dev/null -w "%{http_code}" \
                  http://169.254.169.254/latest/meta-data/public-ipv4)
                PUB_IPV4=$(ec2-metadata -v | awk '{print $2}')
                LOC_IPV4=$(ec2-metadata -o | awk '{print $2}')
                if [ $PUBLIC = "200" ]
                then HOST=$PUB_IPV4
                else HOST=$LOC_IPV4
                fi
                sed -i -e "s~INJECTHOST~$HOST~" /etc/sysconfig/airflow

                sed 's/^/export /' -- </etc/sysconfig/airflow >/tmp/env.sh
                source /tmp/env.sh
                if [ "$TURBINE_MACHINE" == "SCHEDULER" ]
                then if [ "$TURBINE__CORE__LOAD_DEFAULTS" == "True" ]
                  then su -c '/usr/local/bin/airflow initdb' ec2-user
                  else su -c '/usr/local/bin/airflow upgradedb' ec2-user
                  fi
                else echo "Database setup reserved for the scheduler"
                fi

                HAS_DEPLOYMENT=$(aws deploy list-deployments \
                  --application-name ${AWS::StackName}-deployment-application \
                  --deployment-group ${AWS::StackName}-deployment-group | \
                  jq '.deployments | has(0)')

                systemctl enable airflow.service watcher.path

                if [ "$TURBINE_MACHINE" = "WORKER" ]
                then systemctl enable lchkill.timer lchbeat.timer
                fi
                if [ "$TURBINE_MACHINE" = "WORKER" ] && \
                   [ "$HAS_DEPLOYMENT" = "true" ]
                then echo "Deployment pending, deferring service start"
                else systemctl start airflow
                fi

        metahup:
          files:
            /etc/cfn/cfn-hup.conf:
              content: !Sub |
                [main]
                stack=${AWS::StackId}
                region=${AWS::Region}
                interval=1
              mode: '000400'
              owner: root
              group: root
            /etc/cfn/hooks.d/cfn-auto-reloader.conf:
              content: !Sub |
                [cfn-auto-reloader-hook]
                triggers=post.update
                path=Resources.Meta.Metadata.AWS::CloudFormation::Init
                action=/opt/aws/bin/cfn-init -v \
                  --region ${AWS::Region} \
                  --stack ${AWS::StackName} \
                  --resource Meta
                runas=root
            /lib/systemd/system/cfn-hup.service:
              content: |
                [Service]
                ExecStart=/opt/aws/bin/cfn-hup
                Restart=always
                [Install]
                WantedBy=multi-user.target
          commands:
            setup:
              command: |
                systemctl enable cfn-hup.service
                systemctl start cfn-hup.service
        cdagent:
          packages:
            yum:
              ruby: []
              wget: []
          commands:
            install:
              command: !Sub |
                wget https://aws-codedeploy-${AWS::Region}.s3.amazonaws.com/latest/install
                chmod +x ./install
                ./install auto
