AWSTemplateFormatVersion: 2010-09-09
Description: The Airflow cluster stack
Parameters:
  Application:
    Type: String
    Description: The application variable
    Default: Airflow
  Team:
    Type: String
    Description: The team
    Default: Data Team
  Environment:
    Type: String
    Description: The environment variable
    Default: Staging
  Domain:
    Type: String
    Description: The domain suffix
    Default: insights.domain.com.au
  VPC:
    Description: The vpc it is in
    Type: String
    Default: vpc-0900dec7801613f72
  VpcCidrBlock:
    Description: The IPv4 CIDR block to be used in the VPC.
    Type: String
    AllowedPattern: >-
      ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/([0-9]|[1-2][0-9]|3[0-2]))$
    Default: 10.138.32.0/20
  DummySubnetBlock:
    Description: The IPv4 CIDR block to be used in the dummy subnet.
    Type: String
    AllowedPattern: >-
      ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/([0-9]|[1-2][0-9]|3[0-2]))$
    Default: 10.0.99.0/24
  StackSubnet:
    Description: Stack subnet
    Type: String
    Default: subnet-0281813787e9e6370
  DBSubnets:
    Description: Stack subnet
    Type: CommaDelimitedList
    Default: subnet-0281813787e9e6370,subnet-02b6717a06309435c
  StackSubnetBlock:
    Description: The IPv4 CIDR block to be used in the stack subnet.
    Type: String
    AllowedPattern: >-
      ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/([0-9]|[1-2][0-9]|3[0-2]))$
    Default: 10.0.0.0/24
  AllowedSshBlock:
    Description: >-
      The IPv4 CIDR block to allow SSH access in all machines. The default of
      0.0.0.0/0 allows SSH from everywhere, which is convenient but less secure.
    Type: String
    AllowedPattern: >-
      ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/([0-9]|[1-2][0-9]|3[0-2]))$
    Default: 10.0.0.0/8
  AllowedWebBlock:
    Description: >-
      The IPv4 CIDR block to allow HTTP access in the webserver. The default of
      0.0.0.0/0 allows HTTP from everywhere, which is convenient but less
      secure.
    Type: String
    AllowedPattern: >-
      ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/([0-9]|[1-2][0-9]|3[0-2]))$
    Default:  10.0.0.0/8
  WebserverPort:
    Description: >-
      The port Airflow webserver will be listening. Ports below 1024 can be
      opened only with root privileges and the airflow process does not run as
      such.
    Type: String
    Default: 8080
  KeyPair:
    Description: Amazon EC2 Key Pair to be used paired with the EC2 instances.
    Type: 'AWS::EC2::KeyPair::KeyName'
  SchedulerInstanceType:
    Description: EC2 instance type to use for the scheduler.
    Type: String
    Default: t3.medium
  WebserverInstanceType:
    Description: EC2 instance type to use for the webserver.
    Type: String
    Default: t3.small
  WorkerInstanceType:
    Description: EC2 instance type to use for the workers.
    Type: String
    Default: t3.nano
  WorkerConcurrency:
    Description: Number of concurrent units per machine.
    Type: Number
    Default: 3
  MinGroupSize:
    Description: The minimum number of active worker instances.
    Type: Number
    Default: 0
  MaxGroupSize:
    Description: The maximum number of active worker instances.
    Type: Number
    Default: 40
  ShrinkThreshold:
    Description: >-
      The timeout (in seconds, multiple of 60) after which the queue staying
      empty will trigger the AutoScaling group to Scale In, deallocating one
      worker instance.
    Type: Number
    Default: 0.5
  GrowthThreshold:
    Description: >-
      The threshold for the average queue size from which going equal or above
      will trigger the AutoScaling group to Scale Out, allocating one worker
      instance.
    Type: Number
    Default: 0.9
  DbMasterUsername:
    Description: The username to be used in the airflow database.
    Type: String
    Default: airflow
  DbMasterPassword:
    Description: The password to be used in the airflow database.
    Type: String
    NoEcho: true
  LoadExampleDags:
    Description: >-
      Load the example DAGs distributed with Airflow. Useful if deploying a
      stack for demonstrating a few topologies, operators and scheduling
      strategies.
    Type: String
    AllowedValues:
      - 'False'
      - 'True'
    Default: 'False'
  LoadDefaultConns:
    Description: >-
      Load the default connections initialized by Airflow. Most consider these
      unnecessary, which is why the default is to not load them.
    Type: String
    AllowedValues:
      - 'False'
      - 'True'
    Default: 'False'
Resources:
  # VPC:
  #   Type: 'AWS::EC2::VPC'
  #   Properties:
  #     EnableDnsSupport: true
  #     EnableDnsHostnames: true
  #     CidrBlock: !Ref VpcCidrBlock
  #     Tags:
  #       - Key: Name
  #         Value: !Sub '${AWS::StackName}-vpc'
  # DummySubnet:
  #   Type: 'AWS::EC2::Subnet'
  #   Properties:
  #     AvailabilityZone: !Sub '${AWS::Region}b'
  #     CidrBlock: !Ref DummySubnetBlock
  #     Tags:
  #       - Key: Name
  #         Value: !Sub '${AWS::StackName}-dummy'
  #     VpcId: !Ref VPC
  # StackSubnet:
  #   Type: 'AWS::EC2::Subnet'
  #   Properties:
  #     AvailabilityZone: !Sub '${AWS::Region}a'
  #     CidrBlock: !Ref StackSubnetBlock
  #     MapPublicIpOnLaunch: true
  #     Tags:
  #       - Key: Name
  #         Value: !Sub '${AWS::StackName}-subnet'
  #     VpcId: !Ref VPC
   
  # Internet:
  #   Type: 'AWS::EC2::InternetGateway'
  #   Properties: {}
    
  # Routes:
  #   Type: 'AWS::EC2::RouteTable'
  #   Properties:
  #     VpcId: !Ref VPC
    
  # Opener:
  #   Type: 'AWS::EC2::Route'
  #   Properties:
  #     DestinationCidrBlock: 0.0.0.0/0
  #     GatewayId: !Ref Internet
  #     RouteTableId: !Ref Routes
  #   DependsOn:
  #     - VPCGatewayAttachment
    
  # VPCGatewayAttachment:
  #   Type: 'AWS::EC2::VPCGatewayAttachment'
  #   Properties:
  #     InternetGatewayId: !Ref Internet
  #     VpcId: !Ref VPC
    
  # SubnetRouteTableAssociation:
  #   Type: 'AWS::EC2::SubnetRouteTableAssociation'
  #   Properties:
  #     RouteTableId: !Ref Routes
  #     SubnetId: !Ref StackSubnet
    
  AirflowScheduler:
    Type: 'AWS::EC2::Instance'
    Properties:
      InstanceType: !Ref SchedulerInstanceType
      ImageId: ami-0b8dea0e70b969adc # HVMGP2
      SubnetId: !Ref StackSubnet
      KeyName: !Ref KeyPair
      SecurityGroupIds:
        - !Ref Control
      IamInstanceProfile: !Ref AirflowProfile
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-scheduler'
      UserData: !Base64
        'Fn::Sub': |
          #!/bin/bash -xe
          echo $'TURBINE_MACHINE=SCHEDULER' > /etc/environment
          /opt/aws/bin/cfn-init -v \
            --region ${AWS::Region} \
            --stack ${AWS::StackName} \
            --resource Meta
          /opt/aws/bin/cfn-signal -e $?
            --region ${AWS::Region} \
            --stack ${AWS::StackName} \
            --resource AirflowScheduler
    DependsOn:
      - EfsMountTarget
      - Meta
  AirflowWebserver:
    Type: 'AWS::EC2::Instance'
    Properties:
      InstanceType: !Ref WebserverInstanceType
      ImageId: ami-0b8dea0e70b969adc # HVMGP2
      SubnetId: !Ref StackSubnet
      KeyName: !Ref KeyPair
      SecurityGroupIds:
        - !Ref Web
      IamInstanceProfile: !Ref AirflowProfile
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-webserver'
      UserData: !Base64
        'Fn::Sub': |
          #!/bin/bash -xe
          echo $'TURBINE_MACHINE=WEBSERVER' > /etc/environment
          /opt/aws/bin/cfn-init -v \
            --region ${AWS::Region} \
            --stack ${AWS::StackName} \
            --resource Meta
          /opt/aws/bin/cfn-signal -e $?
            --region ${AWS::Region} \
            --stack ${AWS::StackName} \
            --resource AirflowWebserver
    DependsOn:
      - EfsMountTarget
      - Meta
  AirflowWorkerConfig:
    Type: 'AWS::AutoScaling::LaunchConfiguration'
    Properties:
      ImageId: ami-0b8dea0e70b969adc #hvmgp2
      InstanceType: !Ref WorkerInstanceType
      KeyName: !Ref KeyPair
      SecurityGroups:
        - !Ref Comms
      IamInstanceProfile: !Ref AirflowProfile
      UserData: !Base64
        'Fn::Sub': |
          #!/bin/bash -xe
          echo $'TURBINE_MACHINE=WORKER' > /etc/environment
          /opt/aws/bin/cfn-init -v \
            --region ${AWS::Region} \
            --stack ${AWS::StackName} \
            --resource Meta
    
    DependsOn:
      - Meta
  AutoScalingGroup:
    Type: 'AWS::AutoScaling::AutoScalingGroup'
    Properties:
      AutoScalingGroupName: !Sub '${AWS::StackName}-worker-scaling-group'
      LaunchConfigurationName: !Ref AirflowWorkerConfig
      MinSize: !Ref MinGroupSize
      MaxSize: !Ref MaxGroupSize
      MetricsCollection:
        - Granularity: 1Minute
      VPCZoneIdentifier:
        - !Ref StackSubnet
      Tags:
        - Key: Environment
          Value: !Ref Environment
          PropagateAtLaunch: true
        - Key: Application
          Value: !Ref Application
          PropagateAtLaunch: true
        - Key: Team
          Value: !Ref Team
          PropagateAtLaunch: true
        - Key: Name
          Value: !Sub '${AWS::StackName}-worker'
          PropagateAtLaunch: true
  AutoScalingHook:
    Type: 'AWS::AutoScaling::LifecycleHook'
    Properties:
      AutoScalingGroupName: !Ref AutoScalingGroup
      DefaultResult: CONTINUE
      HeartbeatTimeout: 300
      LifecycleHookName: !Sub '${AWS::StackName}-scaling-lfhook'
      LifecycleTransition: 'autoscaling:EC2_INSTANCE_TERMINATING'
  LogsBucket:
    Type: 'AWS::S3::Bucket'
    Properties:
      BucketName: !Sub '${AWS::StackName}-logs.${Domain}'
  DeploymentsBucket:
    Type: 'AWS::S3::Bucket'
    Properties:
      BucketName: !Sub '${AWS::StackName}-deployments.${Domain}'
  CodeDeployApplication:
    Type: 'AWS::CodeDeploy::Application'
    Properties:
      ApplicationName: !Sub '${AWS::StackName}-deployment-application'
      ComputePlatform: Server
  CodeDeployDeploymentGroup:
    Type: 'AWS::CodeDeploy::DeploymentGroup'
    Properties:
      ApplicationName: !Ref CodeDeployApplication
      DeploymentGroupName: !Sub '${AWS::StackName}-deployment-group'
      AutoScalingGroups:
        - !Ref AutoScalingGroup
      Ec2TagSet:
        Ec2TagSetList:
          - Ec2TagGroup:
              - Type: KEY_AND_VALUE
                Key: Name
                Value: !Sub '${AWS::StackName}-scheduler'
              - Type: KEY_AND_VALUE
                Key: Name
                Value: !Sub '${AWS::StackName}-webserver'
      ServiceRoleArn: !GetAtt
        - CodeDeployServiceRole
        - Arn
  CodeDeployServiceRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - codedeploy.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSCodeDeployRole'
  Logger:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      Policies:
        - PolicyName: !Sub '${AWS::StackName}-cloudwatch-policy'
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Resource: '*'
                Action:
                  - 'cloudwatch:GetMetric*'
                  - 'cloudwatch:PutMetricData'
  Metric:
    Type: 'AWS::Lambda::Function'
    Properties:
      Runtime: nodejs8.10
      Handler: index.handler
      Code:
        ZipFile: !Sub
          - |
            var AWS = require('aws-sdk');
            AWS.config.update({region: '${AWS::Region}'});
            var cw = new AWS.CloudWatch({apiVersion: '2010-08-01'});
            const datePlusMinutes = (d, m) => {
              const _d = new Date(d);
              _d.setMinutes(d.getMinutes() + m);
              return _d;
            };
            const getMetricAtTime = (ms, m, t) => {
              const m_idx = ms.MetricDataResults
                .map(_r => _r.Id)
                .indexOf(m);
              const t_idx = ms.MetricDataResults[m_idx]
                .Timestamps
                .map(_t => _t.toISOString())
                .indexOf(t.toISOString());
              return ms.MetricDataResults[m_idx]
                .Values[t_idx];
            };
            const discount = (ms, m, t1, t2, ws) => {
              let incs = 0, d = t1;
              let v1 = getMetricAtTime(ms, m, d), v2;
              for (let i = 0; d < t2 ; i++) {
                d = datePlusMinutes(t1, i+1);
                v2 = getMetricAtTime(ms, m, d);
                if (v2 > v1) incs += ws[i];
                v1 = v2;
              }
              return incs;
            };
            exports.handler = async function(event, context) {
              let curr = new Date();
              curr.setMinutes(Math.floor(curr.getMinutes()/5)*5-5);
              curr.setSeconds(0); curr.setMilliseconds(0);
              const prev = datePlusMinutes(curr, -5);
              const back = datePlusMinutes(prev, -5);
              const metrics = await cw.getMetricData({
                StartTime: back, EndTime: curr,
                ScanBy: 'TimestampDescending',
                MetricDataQueries: [
                  { Id: 'maxANOMV', MetricStat: {
                      Metric: { Namespace: 'AWS/SQS',
                                MetricName: 'ApproximateNumberOfMessagesVisible',
                                Dimensions: [{ Name: 'QueueName',
                                              Value: '${queueName}' }]},
                      Period: 300,
                      Stat: 'Maximum',
                      Unit: 'Count' }},
                  { Id: 'sumNOER', MetricStat: {
                      Metric: { Namespace: 'AWS/SQS',
                                MetricName: 'NumberOfEmptyReceives',
                                Dimensions: [{ Name: 'QueueName',
                                              Value: '${queueName}' }]},
                      Period: 300,
                      Stat: 'Sum',
                      Unit: 'Count', }},
                  { Id: 'avgGISI', MetricStat: {
                      Metric: { Namespace: 'AWS/AutoScaling',
                                MetricName: 'GroupInServiceInstances',
                                Dimensions: [{ Name: 'AutoScalingGroupName',
                                              Value: '${asgName}' }]},
                      Period: 300,
                      Stat: 'Average',
                      Unit: 'None', }},
                  { Id: 'uGISI', MetricStat: {
                      Metric: { Namespace: 'AWS/AutoScaling',
                                MetricName: 'GroupDesiredCapacity',
                                Dimensions: [{ Name: 'AutoScalingGroupName',
                                              Value: '${asgName}' }]},
                      Period: 60,
                      Stat: 'Average',
                      Unit: 'None', }},
              ]}).promise();
              const ANOMV = getMetricAtTime(metrics, 'maxANOMV', prev);
              const NOER = getMetricAtTime(metrics, 'sumNOER', prev);
              const GISI = getMetricAtTime(metrics, 'avgGISI', prev);
              const ws = [0, 0, 0, 0.1, 0.3, 0.3, 0.3, 0.3, 0.2];
              const dGISI = discount(metrics, 'uGISI', back, curr, ws);
              const M = GISI - dGISI;
              let l;
              if (M > 0)
                l = 1 - NOER / (M * 0.098444 * 300);
              else
                l = (ANOMV > 0) ? 1.0 : 0.0;
              await cw.putMetricData({
                Namespace: 'Turbine',
                MetricData: [{ MetricName: 'WorkerLoad',
                              Dimensions: [ { Name: 'StackName',
                                              Value: '${AWS::StackName}' }],
                              Timestamp: prev,
                              Value: (l > 0) ? l : 0,
                              Unit: 'None' }],
              }).promise();
            };
          - asgName: !Sub '${AWS::StackName}-scaling-group'
            queueName: !GetAtt
              - Tasks
              - QueueName
      Role: !GetAtt
        - Logger
        - Arn
  Timer:
    Type: 'AWS::Events::Rule'
    Properties:
      ScheduleExpression: rate(1 minute)
      State: ENABLED
      Targets:
        - Arn: !GetAtt
            - Metric
            - Arn
          Id: TargetFunction
  Invoke:
    Type: 'AWS::Lambda::Permission'
    Properties:
      FunctionName: !Ref Metric
      Action: 'lambda:InvokeFunction'
      Principal: events.amazonaws.com
      SourceArn: !GetAtt
        - Timer
        - Arn
  Shrink:
    Type: 'AWS::CloudWatch::Alarm'
    Properties:
      AlarmActions:
        - !Ref ScaleIn
      Namespace: Turbine
      MetricName: WorkerLoad
      Dimensions:
        - Name: StackName
          Value: !Ref 'AWS::StackName'
      Statistic: Average
      Period: 60
      EvaluationPeriods: 1
      Threshold: !Ref ShrinkThreshold
      ComparisonOperator: LessThanOrEqualToThreshold
  Growth:
    Type: 'AWS::CloudWatch::Alarm'
    Properties:
      AlarmActions:
        - !Ref ScaleOut
      Namespace: Turbine
      MetricName: WorkerLoad
      Dimensions:
        - Name: StackName
          Value: !Ref 'AWS::StackName'
      Statistic: Average
      Period: 60
      EvaluationPeriods: 1
      Threshold: !Ref GrowthThreshold
      ComparisonOperator: GreaterThanOrEqualToThreshold
  ScaleIn:
    Type: 'AWS::AutoScaling::ScalingPolicy'
    Properties:
      AdjustmentType: ChangeInCapacity
      PolicyType: SimpleScaling
      ScalingAdjustment: -1
      Cooldown: '600'
      AutoScalingGroupName: !Ref AutoScalingGroup
  ScaleOut:
    Type: 'AWS::AutoScaling::ScalingPolicy'
    Properties:
      AdjustmentType: ChangeInCapacity
      PolicyType: SimpleScaling
      ScalingAdjustment: 1
      Cooldown: '90'
      AutoScalingGroupName: !Ref AutoScalingGroup
  EfsFileSystem:
    Type: 'AWS::EFS::FileSystem'
    Properties:
      FileSystemTags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-filesystem'
  EfsMountTarget:
    Type: 'AWS::EFS::MountTarget'
    Properties:
      FileSystemId: !Ref EfsFileSystem
      SubnetId: !Ref StackSubnet
      SecurityGroups:
        - !Ref Access
  DBSubnetGroup:
    Type: 'AWS::RDS::DBSubnetGroup'
    Properties:
      DBSubnetGroupDescription: Associates the Database Instances with the selected VPC Subnets.
      SubnetIds: !Ref DBSubnets
  Database:
    Type: 'AWS::RDS::DBInstance'
    DeletionPolicy: Retain
    UpdateReplacePolicy: Retain
    Properties:
      DBInstanceIdentifier: !Sub '${AWS::StackName}-db'
      # EngineMode: serverless # user this when really needs more
      AllocatedStorage: '60'
      DBInstanceClass: db.t2.small
      DBName: airflow
      Engine: postgres
      MasterUsername: !Ref DbMasterUsername
      MasterUserPassword: !Ref DbMasterPassword
      BackupRetentionPeriod: 10
      DeletionProtection: true
      DeleteAutomatedBackups: true
      PreferredBackupWindow: 17:00-18:00
      PreferredMaintenanceWindow: Sun:18:00-Sun:18:30
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-database'
      DBSubnetGroupName: !Ref DBSubnetGroup
      VPCSecurityGroups:
        - !Ref Connection
  Tasks:
    Type: 'AWS::SQS::Queue'
    Properties: {}
  Access:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: >-
        Security Rules with permissions for the shared filesystem across Airflow
        instances.
      SecurityGroupIngress:
        - CidrIp: !Ref VpcCidrBlock
          IpProtocol: TCP
          FromPort: 2049
          ToPort: 2049
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-access'
  Control:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: >-
        Security Rules with permissions for node intercommunication between
        Airflow instances and remote access.
      SecurityGroupIngress:
        - CidrIp: !Ref VpcCidrBlock
          IpProtocol: TCP
          FromPort: 8793
          ToPort: 8793
        - CidrIp: !Ref AllowedSshBlock
          IpProtocol: TCP
          FromPort: 22
          ToPort: 22
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-control'
  Comms:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: >-
        Security Rules with permissions for node itercommunication between
        Airflow worker instances.
      SecurityGroupIngress:
        - CidrIp: !Ref VpcCidrBlock
          IpProtocol: TCP
          FromPort: 8793
          ToPort: 8793
        - CidrIp: !Ref AllowedSshBlock
          IpProtocol: TCP
          FromPort: 22
          ToPort: 22
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-comms'
  Web:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: Security Rules with permissions for the web UI exposed by Airflow.
      SecurityGroupIngress:
        - CidrIp: !Ref AllowedWebBlock
          IpProtocol: TCP
          FromPort: !Ref WebserverPort
          ToPort: !Ref WebserverPort
        - CidrIp: !Ref AllowedSshBlock
          IpProtocol: TCP
          FromPort: 22
          ToPort: 22
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-web'
  Connection:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: Security Rules with permissions for database connections for Airflow.
      SecurityGroupIngress:
        - CidrIp: !Ref VpcCidrBlock
          IpProtocol: TCP
          FromPort: 5432
          ToPort: 5432
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-connection'
  AirflowRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - ec2.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      Policies:
        - PolicyName: !Sub '${AWS::StackName}-queue-rw-policy'
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - 'sqs:ListQueues'
                Resource:
                  - !Sub 'arn:aws:sqs:*:${AWS::AccountId}:*'
              - Effect: Allow
                Action:
                  - 'sqs:ChangeMessageVisibility'
                  - 'sqs:DeleteMessage'
                  - 'sqs:GetQueueAttributes'
                  - 'sqs:GetQueueUrl'
                  - 'sqs:ReceiveMessage'
                  - 'sqs:SendMessage'
                Resource: !Sub
                  - 'arn:aws:sqs:*:${AWS::AccountId}:${queue}'
                  - queue: !GetAtt
                      - Tasks
                      - QueueName
        - PolicyName: !Sub '${AWS::StackName}-deployments-r-policy'
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - 's3:Get*'
                  - 's3:List*'
                Resource: !Sub 'arn:aws:s3:::${DeploymentsBucket}/*'
              - Effect: Allow
                Action:
                  - 'codedeploy:List*'
                Resource: !Sub 'arn:aws:codedeploy:*:${AWS::AccountId}:deploymentgroup:*'
        - PolicyName: !Sub '${AWS::StackName}-logs-rw-policy'
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - 's3:Get*'
                  - 's3:Put*'
                Resource: !Sub 'arn:aws:s3:::${LogsBucket}/*'
        - PolicyName: !Sub '${AWS::StackName}-lifecycle-heartbeat'
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - 'autoscaling:RecordLifecycleActionHeartbeat'
                  - 'autoscaling:CompleteLifecycleAction'
                Resource: !Sub 'arn:aws:autoscaling:*:${AWS::AccountId}:autoScalingGroup:*:*'
              - Effect: Allow
                Action:
                  - 'autoscaling:DescribeScalingActivities'
                Resource: '*'
  AirflowProfile:
    Type: 'AWS::IAM::InstanceProfile'
    Properties:
      Roles:
        - !Ref AirflowRole
  Meta:
    Type: 'AWS::CloudFormation::WaitConditionHandle'
    Properties: {}
    Metadata:
      'AWS::CloudFormation::Init':
        configSets:
          default:
            - filesys
            - runtime
            - service
            - metahup
            - cdagent
        filesys:
          commands:
            mkdir:
              test: test ! -d /airflow
              command: |
                mkdir /airflow
                chown -R ec2-user /airflow
            mount:
              test: test ! "$(mount | grep /mnt/efs)"
              command: !Sub |
                mkdir -p /mnt/efs
                fspec="${EfsFileSystem}.efs.${AWS::Region}.amazonaws.com:/"
                param="nfsvers=4.1,rsize=1048576,wsize=1048576"
                param="$param,hard,timeo=600,retrans=2,noresvport"
                echo "$fspec /mnt/efs nfs $param,_netdev 0 0" > /etc/fstab
                mount /mnt/efs
                chown -R ec2-user /mnt/efs
        runtime:
          packages:
            yum:
              git: []
              gcc: []
              gcc-c++: []
              jq: []
              lapack-devel: []
              libcurl-devel: []
              libxml2-devel: []
              libxslt-devel: []
              openssl-devel: []
              postgresql-devel: []
              python3: []
              python3-devel: []
              python3-pip: []
              python3-wheel: []
          commands:
            install:
              command: |
                PYCURL_SSL_LIBRARY=openssl pip3 install \
                  --no-cache-dir --compile --ignore-installed \
                  pycurl
                SLUGIFY_USES_TEXT_UNIDECODE=yes pip3 install \
                  apache-airflow[celery,postgres,s3]==1.10.2 \
                  celery[sqs] \
                  billiard==3.5.0.4 \
                  tenacity==4.12.0
        service:
          files:
            /usr/bin/turbine:
              mode: 755
              content: |
                #!/bin/sh
                if [ "$TURBINE_MACHINE" == "SCHEDULER" ]
                then exec airflow scheduler
                elif [ "$TURBINE_MACHINE" == "WEBSERVER" ]
                then exec airflow webserver
                elif [ "$TURBINE_MACHINE" == "WORKER" ]
                then exec airflow worker
                else echo "TURBINE_MACHINE value unknown" && exit 1
                fi
            /etc/sysconfig/airflow:
              content: !Sub
                - |
                  AWS_DEFAULT_REGION=${AWS::Region}
                  AIRFLOW_HOME=/airflow
                  AIRFLOW__CORE__EXECUTOR=CeleryExecutor
                  AIRFLOW__CORE__LOAD_EXAMPLES=${LoadExampleDags}
                  TURBINE__CORE__LOAD_DEFAULTS=${LoadDefaultConns}
                  AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql://${DbMasterUsername}:${DbMasterPassword}@${rds}/airflow
                  AIRFLOW__CORE__REMOTE_BASE_LOG_FOLDER=s3://${LogsBucket}
                  AIRFLOW__CORE__REMOTE_LOGGING=True
                  AIRFLOW__WEBSERVER__BASE_URL=http://INJECTHOST:${WebserverPort}
                  AIRFLOW__WEBSERVER__WEB_SERVER_PORT=${WebserverPort}
                  AIRFLOW__CELERY__BROKER_URL=sqs://
                  AIRFLOW__CELERY__DEFAULT_QUEUE=${queue}
                  AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${DbMasterUsername}:${DbMasterPassword}@${rds}/airflow
                  AIRFLOW__CELERY__WORKER_CONCURRENCY=${WorkerConcurrency}
                  AIRFLOW__CELERY_BROKER_TRANSPORT_OPTIONS__REGION=${AWS::Region}
                - queue: !GetAtt
                    - Tasks
                    - QueueName
                  rds: !GetAtt
                    - Database
                    - Endpoint.Address
            /usr/lib/tmpfiles.d/airflow.conf:
              content: |
                D /run/airflow 0755 ec2-user ec2-user
            /usr/lib/systemd/system/airflow.service:
              content: |
                [Service]
                EnvironmentFile=/etc/sysconfig/airflow
                User=ec2-user
                Group=ec2-user
                ExecStart=/usr/bin/turbine
                Restart=always
                RestartSec=5s
                KillMode=mixed
                TimeoutStopSec=24h
                [Install]
                WantedBy=multi-user.target
            /usr/lib/systemd/system/watcher.path:
              content: |
                [Unit]
                After=airflow.service
                PartOf=airflow.service
                [Path]
                PathModified=/etc/sysconfig/airflow
                [Install]
                WantedBy=airflow.service
            /usr/lib/systemd/system/watcher.service:
              content: |
                [Service]
                Type=oneshot
                ExecStartPre=/usr/bin/systemctl daemon-reload
                ExecStart=/usr/bin/systemctl restart airflow
            /usr/bin/lchkill:
              mode: 755
              content: !Sub |
                #!/bin/sh
                INSTANCE_ID=$(ec2-metadata -i | awk '{print $2}')
                TERMINATE_MESSAGE="Terminating EC2 instance: $INSTANCE_ID"
                TERMINATING=$(aws autoscaling describe-scaling-activities \
                  --auto-scaling-group-name '${AWS::StackName}-scaling-group' \
                  --max-items 100 \
                  --region '${AWS::Region}' | \
                  jq --arg TERMINATE_MESSAGE "$TERMINATE_MESSAGE" \
                  '.Activities[]
                  | select(.Description
                  | test($TERMINATE_MESSAGE)) != []')

                if [ "$TERMINATING" = "true" ]
                then
                  systemctl stop airflow
                fi
            /usr/lib/systemd/system/lchkill.timer:
              content: |
                [Timer]
                OnCalendar=*:0/1
                [Install]
                WantedBy=airflow.service
            /usr/lib/systemd/system/lchkill.service:
              content: |
                [Service]
                Type=oneshot
                ExecStart=/usr/bin/lchkill
            /usr/bin/lchbeat:
              mode: 755
              content: !Sub |
                #!/bin/sh
                SERVICE_STATUS=$(systemctl is-active airflow)

                if [ "$SERVICE_STATUS" = "deactivating" ]
                then
                  aws autoscaling record-lifecycle-action-heartbeat \
                    --instance-id $(ec2-metadata -i | awk '{print $2}') \
                    --lifecycle-hook-name '${AWS::StackName}-scaling-lfhook' \
                    --auto-scaling-group-name '${AWS::StackName}-scaling-group' \
                    --region '${AWS::Region}'
                fi
            /usr/lib/systemd/system/lchbeat.timer:
              content: |
                [Timer]
                OnCalendar=*:0/1
                [Install]
                WantedBy=airflow.service
            /usr/lib/systemd/system/lchbeat.service:
              content: |
                [Service]
                Type=oneshot
                ExecStart=/usr/bin/lchbeat
          commands:
            setup:
              command: !Sub |
                cat /etc/environment >> /etc/sysconfig/airflow

                PUBLIC=$(curl -s -o /dev/null -w "%{http_code}" \
                  http://169.254.169.254/latest/meta-data/public-ipv4)
                PUB_IPV4=$(ec2-metadata -v | awk '{print $2}')
                LOC_IPV4=$(ec2-metadata -o | awk '{print $2}')
                if [ $PUBLIC = "200" ]
                then HOST=$PUB_IPV4
                else HOST=$LOC_IPV4
                fi
                sed -i -e "s~INJECTHOST~$HOST~" /etc/sysconfig/airflow

                sed 's/^/export /' -- </etc/sysconfig/airflow >/tmp/env.sh
                source /tmp/env.sh
                if [ "$TURBINE_MACHINE" == "SCHEDULER" ]
                then if [ "$TURBINE__CORE__LOAD_DEFAULTS" == "True" ]
                  then su -c '/usr/local/bin/airflow initdb' ec2-user
                  else su -c '/usr/local/bin/airflow upgradedb' ec2-user
                  fi
                else echo "Database setup reserved for the scheduler"
                fi

                HAS_DEPLOYMENT=$(aws deploy list-deployments \
                  --application-name ${AWS::StackName}-deployment-application \
                  --deployment-group ${AWS::StackName}-deployment-group | \
                  jq '.deployments | has(0)')

                systemctl enable airflow.service watcher.path

                if [ "$TURBINE_MACHINE" = "WORKER" ]
                then systemctl enable lchkill.timer lchbeat.timer
                fi
                if [ "$TURBINE_MACHINE" = "WORKER" ] && \
                   [ "$HAS_DEPLOYMENT" = "true" ]
                then echo "Deployment pending, deferring service start"
                else systemctl start airflow
                fi
        metahup:
          files:
            /etc/cfn/cfn-hup.conf:
              content: !Sub |
                [main]
                stack=${AWS::StackId}
                region=${AWS::Region}
                interval=1
              mode: '000400'
              owner: root
              group: root
            /etc/cfn/hooks.d/cfn-auto-reloader.conf:
              content: !Sub |
                [cfn-auto-reloader-hook]
                triggers=post.update
                path=Resources.Meta.Metadata.AWS::CloudFormation::Init
                action=/opt/aws/bin/cfn-init -v \
                  --region ${AWS::Region} \
                  --stack ${AWS::StackName} \
                  --resource Meta
                runas=root
            /lib/systemd/system/cfn-hup.service:
              content: |
                [Service]
                ExecStart=/opt/aws/bin/cfn-hup
                Restart=always
                [Install]
                WantedBy=multi-user.target
          commands:
            setup:
              command: |
                systemctl enable cfn-hup.service
                systemctl start cfn-hup.service
        cdagent:
          packages:
            yum:
              ruby: []
              wget: []
          commands:
            install:
              command: !Sub |
                wget https://aws-codedeploy-${AWS::Region}.s3.amazonaws.com/latest/install
                chmod +x ./install
                ./install auto
